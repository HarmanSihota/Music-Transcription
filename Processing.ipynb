{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPEN291_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwT9gODeb5u6"
      },
      "source": [
        "# CPEN 291 Final Project\n",
        "\n",
        "#Proposal:\n",
        "Our project would involve turning youtube videos of piano compositions into sheet music. As a person who loves to learn new songs through youtube, I found that it can be often expensive to buy sheet music and instead I try to learn by slowing down the video and looking at the artist's hands. However, that has proven itself as quite a challenge since many times the pianist might edit the video in a way that their fingers aren't visible during all the performance. As a solution, I thought it would be interesting to build an ML system that would take audio as an input (mostly from videos on youtube and other streaming platforms) and create music sheets that would be readily available for download. Upon research, I found that there are quite a few companies that offer similar services, which makes me hopeful of the possibility of implementing my project. There are quite a few articles that mention new exciting ML algorithms such as magenta and Deep Watershed Detection. I would probably rely on a scraping algorithm to get a database with audios from amazing piano performances posted on all sorts of social media. Then, I would sort out the database by converting the audio into something more intelligible (such as waves) that would be analyzed by my model. The model would use the data from the waves to point out what note that is, what's its length, etc. In order to train the model, I could find cheap or free sheet music online in order to have a comparison of what the model generated to what it is originally supposed to look like. Finally, once having an acceptable accuracy rate, I would make the sheet music available on a website where we would be able to download it and learn from it. (Note that this would serve mostly as practice for beginners/intermediate students, as it would not be 100% correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oih6ZQXmlOyd"
      },
      "source": [
        "#Import Statements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAu-2zJZQofM"
      },
      "source": [
        "!pip install pretty_midi\n",
        "import pretty_midi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3E75gp9EQUL"
      },
      "source": [
        "!pip install pydub\n",
        "from pydub import AudioSegment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xtlGKFolQ47"
      },
      "source": [
        "import pandas as pd, csv\n",
        "import torch, torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch import nn, optim, functional as F\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import librosa\n",
        "from librosa import display\n",
        "from IPython.display import Audio,display\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "import PIL\n",
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n3B12hajHbf"
      },
      "source": [
        "# Dataset Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4sTLGq4jKe2"
      },
      "source": [
        "# Code to scrape samples using Selenium. Requires user to install chromedriver.\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "PATH_TO_CHROMEDRIVER = ''\n",
        "\n",
        "driver = webdriver.Chrome(PATH_TO_CHROMEDRIVER)\n",
        "\n",
        "driver.get('https://www.mutopiaproject.org/cgibin/make-table.cgi?Instrument=Piano')\n",
        "\n",
        "for i in range(77):\n",
        "     try:\n",
        "        tables_mid = []\n",
        "        tables_ps = []\n",
        "\n",
        "        for j in range(1,11):\n",
        "            tables_mid.append(driver.find_element_by_xpath(f\"//table/tbody//tr[{j}]/td//table//tbody/tr[4]/td[2]\"))\n",
        "            tables_ps.append(driver.find_element_by_xpath(f\"//table/tbody//tr[{j}]/td//table//tbody/tr[5]/t'd\"))\n",
        "\n",
        "        for table in tables_mid:\n",
        "            time.sleep(0.5)\n",
        "            table.find_element_by_partial_link_text('.mid').click()\n",
        "\n",
        "        for table in tables_ps:\n",
        "            time.sleep(1)\n",
        "            driver.execute_script(\"arguments[0].click();\", table.find_element_by_partial_link_text('.ps'))\n",
        "\n",
        "        link = driver.find_element_by_link_text('Next 10')\n",
        "        link.click()\n",
        "\n",
        "     except:\n",
        "        continue\n",
        "\n",
        "\n",
        "driver.quit()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyW90BNjidEQ"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLEhm_v5Lq-0"
      },
      "source": [
        "\"\"\"\n",
        "This code converts .mid files to .wav\n",
        "\n",
        "Note: the below code requires you to manually install a new version of fluidsynth (using pip install gives you\n",
        "a version that is too old). This code must also be run on a 32-bit version of Python.\n",
        "\"\"\"\n",
        "\n",
        "# import fluidsynth\n",
        "\n",
        "# PATH_TO_SOUNDFONT = ''\n",
        "# PATH_TO_MID = ''\n",
        "# DIR_SAVE = ''\n",
        "\n",
        "# entries = os.listdir(PATH_TO_ENTRIES)\n",
        "\n",
        "# for entry in entries:\n",
        "#   fs = FluidSynth(sample_rate = 22050, sound_font = PATH_TO_SOUNDFONT)\n",
        "#   midi_fn = PATH_TO_MID + '/' + entry\n",
        "#   fs.midi_to_audio(midi_fn, DIR_SAVE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIup2XEdiveS"
      },
      "source": [
        "# inspired by https://stackoverflow.com/questions/5120555/how-can-i-convert-a-wav-from-stereo-to-mono-in-python\n",
        "\n",
        "\"\"\"\n",
        "converts audio files from stereo to mono\n",
        "\"\"\"\n",
        "def convert_to_mono(PATH_WAV):\n",
        "  if os.path.isdir(PATH_WAV):\n",
        "    DIR_WAV = os.listdir(PATH_WAV)\n",
        "    for entry in DIR_WAV:\n",
        "        sound = AudioSegment.from_wav(PATH_WAV + '/' + entry)\n",
        "        sound = sound.set_channels(1)\n",
        "        sound.export(PATH_WAV + '/' + entry, format='wav')\n",
        "  else:\n",
        "    sound = AudioSegment.from_wav(PATH_WAV)\n",
        "    sound = sound.set_channels(1)\n",
        "    sound.export(PATH_WAV, format='wav')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16XGrVhvjK4f"
      },
      "source": [
        "# Model, Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy3vf61ekrUD"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L7wUpCYsFAF"
      },
      "source": [
        "sample_length_2 = 0.5"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-43bGKLR8mY"
      },
      "source": [
        "PATH_SPEC_2 = ''\n",
        "PATH_MID_2 = ''\n",
        "PATH_MODEL = ''"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt5PxB1OrEee"
      },
      "source": [
        "# Inspired by https://github.com/jsleep/wav2mid/blob/master/examples/one_hot.py\n",
        "\n",
        "\"\"\"\n",
        "Creates the labels for a sample given the pretty_midi object of its file, and \n",
        "the start time of the interval. The interval is sample_length_2 seconds long. \n",
        "The label is the first note start or end detected in this interval. If there are\n",
        "multiple, only the first one is returned. If no note starts or ends are detected,\n",
        "the label is 0.\n",
        "\"\"\"\n",
        "\n",
        "def create_label_2(pm, time, fs=1):\n",
        "    for instrument in pm.instruments:\n",
        "        for note in instrument.notes:\n",
        "          if note.start >= (time - sample_length_2) and note.start <= time:\n",
        "            return torch.as_tensor(note.pitch, dtype=torch.long)\n",
        "          elif note.end >= (time - sample_length_2) and note.end <= time:\n",
        "           return torch.as_tensor(note.pitch + 128, dtype=torch.long)\n",
        "\n",
        "    return torch.as_tensor(0, dtype=torch.long)  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zQVIM2jq_KN"
      },
      "source": [
        "\"\"\"\n",
        "Applies CQT to the time signal data of an audio sample and saves and returns the\n",
        "resulting spectrogram.\n",
        "\"\"\"\n",
        "\n",
        "def create_sample_2(signalData, fn, j):\n",
        "    signalData_float = signalData.astype(float)\n",
        "    f = librosa.cqt(signalData_float, fmin=46.25)\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(f, ref=np.max), y_axis='log', x_axis='time', sr=22050)\n",
        "    plt.savefig(PATH_SPEC + '/' + fn.replace('.mid', f'_{j}.jpg'))\n",
        "    return PIL.Image.open(PATH_SPEC + '/' + fn.replace('.mid', f'_{j}.jpg'))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z_kiW_BqscQ"
      },
      "source": [
        "\"\"\"\n",
        "Used only to create the images for the dataset. Given a directory of midi files\n",
        "and their corresponding wav files, creates and saves spectrogram images of the \n",
        "wav files in intervals determined by sample_length_2. If the length of the music\n",
        "specified by the midi file and wav file of a given sample differ, this sample \n",
        "is not added to the dataset.\n",
        "\"\"\"\n",
        "\n",
        "def create_dataset_2(path_mid, path_wav):\n",
        "    entries_mid = os.listdir(path_mid)\n",
        "    entries_mid.sort()\n",
        "    entries_wav = os.listdir(path_wav)\n",
        "    entries_wav.sort()\n",
        "\n",
        "    for i in range(len(entries_mid)):\n",
        "        print(\"itr: \" + str(i))\n",
        "        time = sample_length_2\n",
        "        pm = pretty_midi.PrettyMIDI(path_mid + '/' + entries_mid[i])\n",
        "        samplingFrequency, origSignalData = wavfile.read(path_wav + '/' + entries_wav[i])\n",
        "\n",
        "        if int(len(origSignalData) / samplingFrequency) != int(pm.get_end_time()):\n",
        "            print(\"different length, not saved\")\n",
        "            print(\"duration of \" + entries_wav[i] + \": \" + str(int(len(origSignalData) / samplingFrequency)))\n",
        "            print(\"duration of \" + entries_mid[i] + \": \" + str(int(pm.get_end_time())))\n",
        "            continue\n",
        "\n",
        "        j = 0\n",
        "        while time < pm.get_end_time():\n",
        "            if samplingFrequency != 22050:\n",
        "                print(\"bad sampling freq: \" + str(samplingFrequency))\n",
        "                return\n",
        "\n",
        "            signalData = origSignalData[int((samplingFrequency * (time - sample_length_2))):int(time * samplingFrequency)]\n",
        "            sample = create_sample_2(signalData, entries_mid[i], j)\n",
        "            time += sample_length_2\n",
        "            j += 1\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCHq8cmmJUok"
      },
      "source": [
        "\"\"\"\n",
        "Fetches spectrogram images and labels to create the dataset. Applies the given \n",
        "transform to the images.\n",
        "\"\"\"\n",
        "\n",
        "def get_dataset_2(path_mid, path_spec, transform):\n",
        "    entries_mid = os.listdir(path_mid)\n",
        "    entries_mid.sort()\n",
        "    entries_spec = os.listdir(path_spec)\n",
        "    entries_spec.sort()\n",
        "    dataset = []\n",
        "\n",
        "    for i in range(len(entries_mid)):\n",
        "        print(\"itr: \" + str(i))\n",
        "        time = sample_length_2\n",
        "        pm = pretty_midi.PrettyMIDI(path_mid + '/' + entries_mid[i])\n",
        "\n",
        "        j = 0\n",
        "        while float(time) < pm.get_end_time():\n",
        "            sample = PIL.Image.open(path_spec + '/' + entries_mid[i].replace('.mid', f'_{j}.jpg'))\n",
        "            sample = sample.crop((81, 59, 576, 427))\n",
        "            sample = transform(sample)\n",
        "            label = create_label_2(pm, time)\n",
        "            dataset.append((sample, label))\n",
        "            time += sample_length_2\n",
        "            j += 1\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCLuMsaKJ26J"
      },
      "source": [
        "\"\"\"\n",
        "Class for dataset. Calls get_dataset_2 to fetch the images and \n",
        "create the corresponding labels. Passes in a transform to resize the images\n",
        "and convert them to tensors\n",
        "\"\"\"\n",
        "\n",
        "class Dataset_2():\n",
        "  def __init__(self, PATH_MID, PATH_SPEC):\n",
        "    transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
        "    self.dataset = get_dataset_2(PATH_MID, PATH_SPEC, transform)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    if torch.is_tensor(i):\n",
        "      i = i.item()\n",
        "\n",
        "    return self.dataset[i][0], self.dataset[i][1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TEBZhziPkOw"
      },
      "source": [
        "dataset_2 = Dataset_2(PATH_MID_2, PATH_SPEC_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UocY6CUUQLdy"
      },
      "source": [
        "n_all_2 = len(dataset_2)\n",
        "n_train_2 = int(0.8 * n_all_2)\n",
        "n_test_2 = n_all_2 - n_train_2\n",
        "rng = torch.Generator().manual_seed(1711)\n",
        "dataset_train_2, dataset_test_2 = torch.utils.data.random_split(dataset_2, [n_train_2, n_test_2], rng)\n",
        "\n",
        "ldr_train_2 = torch.utils.data.DataLoader(dataset_train_2, batch_size=128, shuffle=True)\n",
        "ldr_test_2 = torch.utils.data.DataLoader(dataset_test_2, batch_size=128, shuffle=True)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyq4vz3fKEnE"
      },
      "source": [
        "model_2 = models.resnet18(pretrained=True)\n",
        "model_2.fc = nn.Linear(512, 257)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRYTOww0nB0k"
      },
      "source": [
        "# For model not pretrained on our data\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == torch.nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "model_2.fc.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrmemXdpQFA2"
      },
      "source": [
        "# For Model pretrained on our data\n",
        "\n",
        "\"\"\"\n",
        "model_2 = torch.load(PATH_MODEL)\n",
        "model_2.eval()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtti9pReK3xf"
      },
      "source": [
        "criterion_2 = nn.CrossEntropyLoss()\n",
        "optimizer_2 = optim.AdamW(model_2.parameters(), lr=0.001, weight_decay=2)\n",
        "scheduler_2 = optim.lr_scheduler.StepLR(optimizer_2, step_size=5, gamma=0.2)\n",
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUBGK3OSQg_Q"
      },
      "source": [
        "\"\"\"\n",
        "Training and testing code. The accuracy computation assumes cross entropy loss\n",
        "is used for a classification task.\n",
        "\"\"\"\n",
        "\n",
        "def run_train_2(model, opt, sched, criterion):\n",
        "    nsamples_train = len(dataset_train_2)\n",
        "    loss_sofar, correct_sofar = 0, 0\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    with torch.enable_grad():\n",
        "        for samples, labels in ldr_train_2:\n",
        "            model.zero_grad()\n",
        "            samples = samples.to(device)\n",
        "            labels = labels.to(device)\n",
        "            opt.zero_grad()\n",
        "            outs = model(samples)\n",
        "            loss = criterion(outs, labels)\n",
        "            _, preds = torch.max(outs.detach(), 1)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss_sofar += loss.item() * samples.size(0)\n",
        "            correct_sofar += torch.sum(preds == labels.detach())\n",
        "    sched.step()\n",
        "    return loss_sofar / nsamples_train, correct_sofar / nsamples_train\n",
        "\n",
        "\n",
        "def run_test_2(model, criterion):\n",
        "    nsamples_test = len(dataset_test_2)\n",
        "    loss, correct = 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for samples, labels in ldr_test_2:\n",
        "            samples = samples.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outs = model(samples)\n",
        "            loss += criterion(outs, labels) * samples.size(0)\n",
        "            _, preds = torch.max(outs.detach(), 1)\n",
        "            correct_mask = preds == labels\n",
        "            correct += correct_mask.sum(0).item()\n",
        "    return loss / nsamples_test, correct / nsamples_test\n",
        "\n",
        "\n",
        "def run_all_2(model, optimizer, scheduler, criterion, n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "        loss_train, acc_train = run_train_2(model, optimizer, scheduler, criterion)\n",
        "        loss_test, acc_test = run_test_2(model, criterion)\n",
        "        print(f\"epoch {epoch}: train loss {loss_train:.4f} acc {acc_train:.4f}, test loss {loss_test:.4f} acc {acc_test:.4f}\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW_P0JBIKZ2o"
      },
      "source": [
        "run_all_2(model_2, optimizer_2, scheduler_2, criterion_2, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym8E1L28Xa3o"
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTGK8GFI65t7"
      },
      "source": [
        "\"\"\"\n",
        "Helper Function that creates a spectrogram for a portion of an audio file.\n",
        "\"\"\"\n",
        "\n",
        "def create_sample_post(signalData, fn, j):\n",
        "    signalData_float = signalData.astype(float)\n",
        "    f = librosa.cqt(signalData_float, fmin=46.25)\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(f, ref=np.max), y_axis='log', x_axis='time', sr=22050)\n",
        "    plt.savefig(fn.replace('.wav', f'_{j}.jpg'))\n",
        "    image =  PIL.Image.open(fn.replace('.wav', f'_{j}.jpg'))\n",
        "    os.remove(fn.replace('.wav', f'_{j}.jpg'))\n",
        "    return image"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2gPbHiOXeS9"
      },
      "source": [
        "\"\"\"\n",
        "Helper function to accept the user's audio file input and split it \n",
        "into segments to be fed to model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def split_audio(audio_file):\n",
        "  transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
        "  time = sample_length_2\n",
        "  samplingFrequency, origSignalData = wavfile.read(audio_file)\n",
        "  audio_length = int(len(origSignalData) / samplingFrequency)\n",
        "  audio_split = []\n",
        "\n",
        "  j = 0\n",
        "  while time < audio_length:\n",
        "    signalData = origSignalData[int((samplingFrequency * (time - sample_length_2))):int(time * samplingFrequency)]\n",
        "    sample = create_sample_post(signalData, audio_file, j)\n",
        "    sample = sample.crop((81, 59, 576, 427))\n",
        "    sample = transform(sample)\n",
        "    sample = sample[None,:,:,:]\n",
        "    audio_split.append(sample)\n",
        "    time += sample_length_2\n",
        "    j += 1\n",
        "\n",
        "  if time - audio_length > 0:\n",
        "    signalData = origSignalData[(samplingFrequency*(time - sample_length_2)):]\n",
        "    sample = create_sample_post(signalData, audio_file, j)\n",
        "    sample = sample.crop((81, 59, 576, 427))\n",
        "    sample = transform(sample)\n",
        "    sample = sample[None,:,:,:]\n",
        "    audio_split.append(sample)\n",
        "\n",
        "  return audio_split\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izCrn-V3bPjQ"
      },
      "source": [
        "\"\"\"\n",
        "Helper function that accepts users' audio file and returns the predicted \n",
        "notes in small intervals.\n",
        "\"\"\"\n",
        "\n",
        "def get_notes(audio_file):\n",
        "  model_2.eval()\n",
        "  audio_split = split_audio(audio_file)\n",
        "  notes = []\n",
        "\n",
        "  for segment in audio_split:\n",
        "    segment = segment.to(device)\n",
        "    output = model_2(segment)\n",
        "    _, preds = torch.max(output.detach(), 1)\n",
        "    notes.append(preds.item())\n",
        "\n",
        "  return notes"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrol9ZvSpt79"
      },
      "source": [
        "\"\"\"\n",
        "Helper function that converts m4a audio files to wav files\n",
        "\"\"\"\n",
        "\n",
        "def convert_to_wav(path):\n",
        "  AudioSegment.from_file(path).export(path.replace('.m4a', '.wav'), format='wav')\n",
        "  return path.replace('.m4a', '.wav')\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmjuXfTocvCb"
      },
      "source": [
        "\"\"\"\n",
        "Function creates a midi file from a wav file using the model. Since the accuracy\n",
        "of the model is not perfect, when a note start is detected the next note end \n",
        "that is a similar pitch is used as the corresponding note end. Since the model\n",
        "does not preserve the information about the instrument played, the output file\n",
        "is assumed to be played by the Acoustic Grand Piano.\n",
        "\"\"\"\n",
        "\n",
        "def create_midi(path_input, path_output):\n",
        "  path_wav = convert_to_wav(path_input)\n",
        "  convert_to_mono(path_wav)\n",
        "  notes = get_notes(path_wav)\n",
        "  mid_file = pretty_midi.PrettyMIDI()\n",
        "  piano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
        "  piano = pretty_midi.Instrument(program=piano_program)\n",
        "\n",
        "  time = 0.0\n",
        "  for i in range(len(notes)):\n",
        "    if notes[i] <= 128:\n",
        "      onset = notes[i]\n",
        "      for j in range(i, len(notes)):\n",
        "        if onset + 120 <= notes[j] <= onset + 136 :\n",
        "          note = pretty_midi.Note(velocity=100, pitch=onset, start=i, end=j)\n",
        "          piano.notes.append(note)\n",
        "          break\n",
        "\n",
        "  mid_file.instruments.append(piano)\n",
        "  mid_file.write(path_output)"
      ],
      "execution_count": 105,
      "outputs": []
    }
  ]
}